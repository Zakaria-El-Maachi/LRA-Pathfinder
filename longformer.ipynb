{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10138639,"sourceType":"datasetVersion","datasetId":6257264}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports and Configuration","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom typing import Optional, Tuple\nfrom pathlib import Path\nimport h5py\nimport math\n\n\n\n# # Set up reproducibility and basic configuration\n# pl.seed_everything(42)\n# EXPERIMENT_NAME = f\"sequential_pathfinder_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n\n# # Model and training hyperparameters\n# HIDDEN_SIZE = 256\n# NUM_LAYERS = 4\n# NUM_HEADS = 8\n# DROPOUT = 0.1\n# BATCH_SIZE = 64\n# LEARNING_RATE = 1e-4\n# MAX_EPOCHS = 35\n# WINDOW_SIZE = 5  # Size of local view window (5x5)\n# MAX_STEPS = 128  # Maximum sequence length for path following\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:22:11.467877Z","iopub.execute_input":"2024-12-09T03:22:11.468237Z","iopub.status.idle":"2024-12-09T03:22:11.474449Z","shell.execute_reply.started":"2024-12-09T03:22:11.468194Z","shell.execute_reply":"2024-12-09T03:22:11.473657Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Sequential Dataset Implementation","metadata":{}},{"cell_type":"code","source":"class PathfinderH5Dataset(Dataset):\n    def __init__(self, h5_path):\n        self.h5_path = h5_path\n        with h5py.File(h5_path, 'r') as f:\n            self.length = len(f['images'])\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, idx):\n        with h5py.File(self.h5_path, 'r') as f:\n            image = torch.from_numpy(f['images'][idx]).float()\n            label = torch.tensor(f['labels'][idx]).long()\n        \n        image = (image > 127.5).float()\n        directions = self.compute_direction_embeddings(image)\n        pos_emb = self.get_positional_encoding(1024, 256)\n        \n        return {\n            'image': image.view(-1),\n            'directions': directions,\n            'pos_emb': pos_emb,\n            'label': label\n        }\n    \n    def compute_direction_embeddings(self, image):\n        dirs = torch.zeros((32, 32, 8))\n        padded = F.pad(image, (1, 1, 1, 1))\n        directions = [(-1, -1), (-1, 0), (-1, 1), (0, -1), \n                     (0, 1), (1, -1), (1, 0), (1, 1)]\n        \n        for i in range(32):\n            for j in range(32):\n                if image[i, j] > 0:\n                    for d, (di, dj) in enumerate(directions):\n                        dirs[i, j, d] = padded[i+di+1, j+dj+1]\n        return dirs.view(-1, 8)\n    \n    def get_positional_encoding(self, seq_len, d_model):\n        position = torch.arange(seq_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(seq_len, d_model)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        return pe[:, :128]  # Match d_model dimension","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:22:11.476203Z","iopub.execute_input":"2024-12-09T03:22:11.476442Z","iopub.status.idle":"2024-12-09T03:22:11.493040Z","shell.execute_reply.started":"2024-12-09T03:22:11.476418Z","shell.execute_reply":"2024-12-09T03:22:11.492367Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"##  Attention Modules\n","metadata":{}},{"cell_type":"code","source":"class LocalGlobalAttention(nn.Module):\n    def __init__(self, dim: int, window_size: int = 7, num_heads: int = 8):\n        super().__init__()\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        \n        self.qkv = nn.Linear(dim, dim * 3)\n        self.proj = nn.Linear(dim, dim)\n        \n    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Compute attention scores\n        scale = self.head_dim ** -0.5\n        attn = (q @ k.transpose(-2, -1)) * scale\n        \n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, float('-inf'))\n        \n        attn = F.softmax(attn, dim=-1)\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:22:11.493932Z","iopub.execute_input":"2024-12-09T03:22:11.494204Z","iopub.status.idle":"2024-12-09T03:22:11.508933Z","shell.execute_reply.started":"2024-12-09T03:22:11.494176Z","shell.execute_reply":"2024-12-09T03:22:11.508128Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"##  Transformer","metadata":{}},{"cell_type":"code","source":"# Main Model\nclass PathfinderTransformer(pl.LightningModule):\n    def __init__(\n        self,\n        d_model: int = 128,\n        nhead: int = 4,\n        num_layers: int = 4,\n    ):\n        super().__init__()\n        \n        self.pixel_embed = nn.Linear(1, d_model)\n        self.dir_embed = nn.Linear(8, d_model)\n        self.pos_embed = nn.Linear(d_model, d_model)\n        \n        self.layers = nn.ModuleList([\n            LocalGlobalAttention(d_model, window_size=7, num_heads=nhead)\n            for _ in range(num_layers)\n        ])\n        \n        self.norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(d_model, 2)\n        \n    def forward(self, batch):\n        x = batch['image'].unsqueeze(-1)\n        dirs = batch['directions']\n        pos = batch['pos_emb']\n        \n        # Embeddings\n        x = self.pixel_embed(x)\n        dirs = self.dir_embed(dirs)\n        pos = self.pos_embed(pos)\n        \n        # Combine embeddings\n        x = x + dirs + pos\n        x = self.dropout(x)\n        \n        # Process through transformer layers\n        for layer in self.layers:\n            x = layer(x)\n        \n        x = self.norm(x)\n        x = x.mean(dim=1)  # Global pooling\n        x = self.classifier(x)\n        \n        return x\n    \n    def training_step(self, batch, batch_idx):\n        y_hat = self(batch)\n        loss = F.cross_entropy(y_hat, batch['label'])\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        y_hat = self(batch)\n        loss = F.cross_entropy(y_hat, batch['label'])\n        acc = (y_hat.argmax(dim=1) == batch['label']).float().mean()\n        self.log('val_loss', loss)\n        self.log('val_acc', acc)\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(),\n            lr=1e-4,\n            weight_decay=0.01\n        )\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max=200,\n            eta_min=1e-6\n        )\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\"\n            }\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:22:11.510097Z","iopub.execute_input":"2024-12-09T03:22:11.510381Z","iopub.status.idle":"2024-12-09T03:22:11.522102Z","shell.execute_reply.started":"2024-12-09T03:22:11.510357Z","shell.execute_reply":"2024-12-09T03:22:11.521308Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Training Setup","metadata":{}},{"cell_type":"code","source":"def train_pathfinder_model():\n    \"\"\"\n    Trains the Sequential Longformer model on the pathfinder task,\n    implementing curriculum learning across different difficulty levels.\n    \"\"\"\n    # Initialize data module with all difficulty levels\n    data_module = PathfinderDataModule('/kaggle/input/dataset-longformer')\n    data_module.setup()\n    \n    model = SequentialLongformerModel()\n    dummy_input = torch.randn(BATCH_SIZE, MAX_STEPS, model.input_size)\n    dummy_mask = torch.ones(BATCH_SIZE, MAX_STEPS)\n    output = model(dummy_input, dummy_mask)\n    print(output.shape)  # Should match (BATCH_SIZE, 2)\n\n    \n    # Create callbacks for monitoring and saving\n    callbacks = [\n        pl.callbacks.ModelCheckpoint(\n            dirpath='checkpoints',\n            filename='pathfinder-{epoch:02d}-{val_acc:.2f}',\n            monitor='val_acc',\n            mode='max',\n            save_top_k=3,\n            verbose=True\n        ),\n        pl.callbacks.EarlyStopping(\n            monitor='val_acc',\n            mode='max',\n            patience=7,    # More patience for complex learning\n            min_delta=0.01,\n            verbose=True\n        ),\n        pl.callbacks.LearningRateMonitor(\n            logging_interval='step'\n        )\n    ]\n    \n    # Set up logger for detailed training monitoring\n    logger = pl.loggers.TensorBoardLogger(\n        save_dir='logs',\n        name=EXPERIMENT_NAME,\n        version=datetime.now().strftime('%Y%m%d_%H%M')\n    )\n    \n    # Initialize trainer with our configurations\n    trainer = pl.Trainer(\n        max_epochs=5,\n        accelerator='gpu',\n        devices=1,\n        precision='16-mixed',\n        callbacks=callbacks,\n        logger=logger,\n        gradient_clip_val=0.5,\n        accumulate_grad_batches=2,  # Effective batch size doubling\n        log_every_n_steps=10\n    )\n    \n    # Train progressively on each difficulty level\n    difficulties = ['easy', 'medium', 'hard']\n    results = {}\n    \n    for difficulty in difficulties:\n        print(f\"\\nTraining on {difficulty.upper()} dataset:\")\n        print(\"=\" * 50)\n        \n        # Train model\n        trainer.fit(\n            model,\n            train_dataloaders=data_module.train_dataloader(difficulty),\n            val_dataloaders=data_module.val_dataloader(difficulty)\n        )\n        \n        # Test performance\n        test_results = trainer.test(\n            model,\n            dataloaders=data_module.test_dataloader(difficulty)\n        )\n        \n        results[difficulty] = test_results[0]\n        \n        print(f\"\\nResults for {difficulty}:\")\n        print(f\"Test Accuracy: {results[difficulty]['test_acc']*100:.2f}%\")\n        print(f\"Test Loss: {results[difficulty]['test_loss']:.4f}\")\n        \n        # Save model state for this difficulty level\n        torch.save(\n            model.state_dict(),\n            f'model_state_{difficulty}.pt'\n        )\n    \n    return model, results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:22:11.603268Z","iopub.execute_input":"2024-12-09T03:22:11.603527Z","iopub.status.idle":"2024-12-09T03:22:11.612163Z","shell.execute_reply.started":"2024-12-09T03:22:11.603503Z","shell.execute_reply":"2024-12-09T03:22:11.611290Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Progressive Training function ","metadata":{}},{"cell_type":"code","source":"def train_progressive(max_epochs: int = 5):\n    from torch.utils.tensorboard import SummaryWriter\n    writer = SummaryWriter('runs/pathfinder')\n    histories = {'medium': [], 'hard': []}\n    \n    difficulties = ['medium', 'hard']\n    model = None\n    \n    for difficulty in difficulties:\n        print(f\"\\nTraining on {difficulty} dataset...\")\n        file_path = f'/kaggle/input/dataset-longformer/merged_data_{difficulty}.h5'\n        \n        with h5py.File(file_path, 'r') as f:\n            total_samples = len(f['images'])\n            print(f\"\\nDataset size: {total_samples}\")\n            print(f\"Connected paths: {(f['labels'][:] == 1).sum()}\")\n            print(f\"Disconnected paths: {(f['labels'][:] == 0).sum()}\")\n            total_samples = len(f['images'])\n            indices = torch.randperm(total_samples)\n            train_split = int(0.8 * total_samples)\n            val_split = int(0.9 * total_samples)\n            \n            train_dataset = PathfinderH5Dataset(file_path)\n            val_dataset = PathfinderH5Dataset(file_path)\n            test_dataset = PathfinderH5Dataset(file_path)\n            \n            train_dataset.indices = indices[:train_split]\n            val_dataset.indices = indices[train_split:val_split]\n            test_dataset.indices = indices[val_split:]\n        \n        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)\n        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n        \n        if model is None:\n            model = PathfinderTransformer(d_model=128, nhead=4, num_layers=4)\n        \n        trainer = pl.Trainer(\n            max_epochs=max_epochs,\n            accelerator='gpu',\n            devices=1,\n            precision=16,\n            limit_train_batches=0.3,\n            limit_val_batches=0.2,\n            callbacks=[\n               pl.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n               pl.callbacks.RichProgressBar(),\n               pl.callbacks.LearningRateMonitor()\n           ],\n            enable_progress_bar=True,\n            log_every_n_steps=1,\n            logger=pl.loggers.TensorBoardLogger('runs', name=f'pathfinder_{difficulty}')\n\n        )\n        class MetricsCallback(pl.Callback):\n           def on_train_epoch_end(self, trainer, pl_module):\n               metrics = trainer.callback_metrics\n               histories[difficulty].append({\n                   'epoch': trainer.current_epoch,\n                   'train_loss': metrics['train_loss'].item(),\n                   'train_acc': metrics['train_acc'].item(),\n                   'val_loss': metrics['val_loss'].item(),\n                   'val_acc': metrics['val_acc'].item()\n               })\n               print(f\"\\nEpoch {trainer.current_epoch}\")\n               print(f\"Train Loss: {metrics['train_loss']:.4f}, Acc: {metrics['train_acc']:.4f}\")\n               print(f\"Val Loss: {metrics['val_loss']:.4f}, Acc: {metrics['val_acc']:.4f}\")\n               \n               writer.add_scalar(f'{difficulty}/train_loss', metrics['train_loss'], trainer.current_epoch)\n               writer.add_scalar(f'{difficulty}/train_acc', metrics['train_acc'], trainer.current_epoch)\n               writer.add_scalar(f'{difficulty}/val_loss', metrics['val_loss'], trainer.current_epoch)\n               writer.add_scalar(f'{difficulty}/val_acc', metrics['val_acc'], trainer.current_epoch)\n       \n        trainer.callbacks.append(MetricsCallback())\n        trainer.fit(model, train_loader, val_loader)\n        test_result = trainer.test(model, test_loader)\n        print(f\"\\nTest results for {difficulty}: {test_result}\")\n        \n    plot_histories(histories)\n    writer.close()\n    return model\n\ndef plot_histories(histories):\n   fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n   \n   for difficulty in histories:\n       data = pd.DataFrame(histories[difficulty])\n       data.plot(x='epoch', y='train_loss', ax=axes[0,0], label=f'{difficulty}_train')\n       data.plot(x='epoch', y='val_loss', ax=axes[0,1], label=f'{difficulty}_val')\n       data.plot(x='epoch', y='train_acc', ax=axes[1,0], label=f'{difficulty}_train')\n       data.plot(x='epoch', y='val_acc', ax=axes[1,1], label=f'{difficulty}_val')\n   \n   axes[0,0].set_title('Training Loss')\n   axes[0,1].set_title('Validation Loss')\n   axes[1,0].set_title('Training Accuracy')\n   axes[1,1].set_title('Validation Accuracy')\n   \n   plt.tight_layout()\n   plt.savefig('training_history.png')\n   plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:22:11.613768Z","iopub.execute_input":"2024-12-09T03:22:11.614043Z","iopub.status.idle":"2024-12-09T03:22:11.629424Z","shell.execute_reply.started":"2024-12-09T03:22:11.614019Z","shell.execute_reply":"2024-12-09T03:22:11.628439Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model = train_progressive()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:22:11.630283Z","iopub.execute_input":"2024-12-09T03:22:11.630540Z"}},"outputs":[{"name":"stdout","text":"\nTraining on medium dataset...\n\nDataset size: 200000\nConnected paths: 100222\nDisconnected paths: 99778\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning_fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"┏━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName       \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n┡━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ pixel_embed │ Linear     │    256 │ train │\n│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ dir_embed   │ Linear     │  1.2 K │ train │\n│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ pos_embed   │ Linear     │ 16.5 K │ train │\n│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ layers      │ ModuleList │  264 K │ train │\n│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ norm        │ LayerNorm  │    256 │ train │\n│\u001b[2m \u001b[0m\u001b[2m5\u001b[0m\u001b[2m \u001b[0m│ dropout     │ Dropout    │      0 │ train │\n│\u001b[2m \u001b[0m\u001b[2m6\u001b[0m\u001b[2m \u001b[0m│ classifier  │ Linear     │    258 │ train │\n└───┴─────────────┴────────────┴────────┴───────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name        </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n┡━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ pixel_embed │ Linear     │    256 │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ dir_embed   │ Linear     │  1.2 K │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ pos_embed   │ Linear     │ 16.5 K │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ layers      │ ModuleList │  264 K │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ norm        │ LayerNorm  │    256 │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span>│ dropout     │ Dropout    │      0 │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>│ classifier  │ Linear     │    258 │ train │\n└───┴─────────────┴────────────┴────────┴───────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mTrainable params\u001b[0m: 282 K                                                                                            \n\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n\u001b[1mTotal params\u001b[0m: 282 K                                                                                                \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 1                                                                          \n\u001b[1mModules in train mode\u001b[0m: 19                                                                                          \n\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 282 K                                                                                            \n<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total params</span>: 282 K                                                                                                \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 1                                                                          \n<span style=\"font-weight: bold\">Modules in train mode</span>: 19                                                                                          \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57d87aa9e3c3455993ce1ea0caf6dce8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The \n'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \n`num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The \n'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \n`num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The \n'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \n`num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The \n'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the \n`num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n</pre>\n"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# In separate cell after training starts:\n%load_ext tensorboard\n%tensorboard --logdir runs/","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}