{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10138003,"sourceType":"datasetVersion","datasetId":6256839},{"sourceId":10139303,"sourceType":"datasetVersion","datasetId":6257773},{"sourceId":10139667,"sourceType":"datasetVersion","datasetId":6258059}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport h5py\n\nclass SequenceDataset(Dataset):\n    def __init__(self, h5_file):\n        self.h5_file = h5py.File(h5_file, \"r\")\n        self.images = self.h5_file[\"images\"]  # Shape: (N, 32, 32)\n        self.labels = self.h5_file[\"labels\"]  # Shape: (N,)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        # Get image and reshape to a sequence of 1024 pixels\n        image = self.images[idx]  # Shape: (32, 32)\n        image = image.flatten()  # Shape: (1024,)\n        image = torch.tensor(image, dtype=torch.float32).unsqueeze(-1)  # Shape: (1024, 1)\n        \n        # Get label\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        \n        return image, label\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T07:29:13.616338Z","iopub.execute_input":"2024-12-09T07:29:13.616674Z","iopub.status.idle":"2024-12-09T07:29:13.752072Z","shell.execute_reply.started":"2024-12-09T07:29:13.616642Z","shell.execute_reply":"2024-12-09T07:29:13.751326Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Hyperparameters\nvocab_size = 256  # Pixel values range from 0 to 255\nembedding_dim = 64\nbatch_size = 32\nlearning_rate = 0.001\nepochs = 10\ntest_size = 0.2  # Fraction of the dataset to use for testing\n\n# Load the dataset\nfull_dataset = SequenceDataset(\"/kaggle/input/easydata/merged_data (3).h5\")\n\n# Calculate the sizes of the train and test sets\ntrain_size = int((1 - test_size) * len(full_dataset))\ntest_size = len(full_dataset) - train_size\n\n# Split the dataset\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T07:29:37.698325Z","iopub.execute_input":"2024-12-09T07:29:37.699267Z","iopub.status.idle":"2024-12-09T07:29:37.731602Z","shell.execute_reply.started":"2024-12-09T07:29:37.699203Z","shell.execute_reply":"2024-12-09T07:29:37.730739Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"test_size2 = 0.2\nfull_dataset2 = SequenceDataset(\"/kaggle/input/mediumdata/merged_data (2).h5\")\n\n# Calculate the sizes of the train and test sets\ntrain_size2 = int((1 - test_size2) * len(full_dataset2))\ntest_size2 = len(full_dataset2) - train_size2\n\n# Split the dataset\ntrain_dataset2, test_dataset2 = random_split(full_dataset2, [train_size2, test_size2])\n\n# Create data loaders\ntrain_dataloader2 = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\ntest_dataloader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T20:25:56.008062Z","iopub.execute_input":"2024-12-08T20:25:56.008775Z","iopub.status.idle":"2024-12-08T20:25:56.030329Z","shell.execute_reply.started":"2024-12-08T20:25:56.008732Z","shell.execute_reply":"2024-12-08T20:25:56.029504Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"full_dataset3= SequenceDataset(\"/kaggle/input/harddata/merged_data (1).h5\")\ntest_size3 = 0.2\n# Calculate the sizes of the train and test sets\ntrain_size3 = int((1 - test_size3) * len(full_dataset3))\ntest_size3 = len(full_dataset3) - train_size3\n\n# Split the dataset\ntrain_dataset3, test_dataset3 = random_split(full_dataset3, [train_size3, test_size3])\n\n# Create data loaders\ntrain_dataloader3 = DataLoader(train_dataset3, batch_size=batch_size, shuffle=True)\ntest_dataloader3= DataLoader(test_dataset3, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T07:29:43.594666Z","iopub.execute_input":"2024-12-09T07:29:43.595462Z","iopub.status.idle":"2024-12-09T07:29:43.625213Z","shell.execute_reply.started":"2024-12-09T07:29:43.595425Z","shell.execute_reply":"2024-12-09T07:29:43.624429Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"!pip install performer-pytorch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom performer_pytorch import Performer\nfrom tqdm import tqdm  # Import tqdm for progress bars\n\nclass PerformerModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, num_classes=2, dim_head=64):\n        super(PerformerModel, self).__init__()\n        \n        # Embedding layer for pixel values\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # Performer Model\n        self.performer = Performer(\n            dim=embedding_dim,\n            depth=4,  # Number of layers\n            heads=8,  # Number of attention heads\n            dim_head=dim_head,  # Size of each attention head\n            kernel_fn=\"relu\",  # You can use 'relu' or 'cosine'\n            causal=False\n        )\n        \n        # Final classification layer (after performer output)\n        self.fc = nn.Linear(embedding_dim, num_classes)\n        \n    def forward(self, x):\n        # x has shape (batch_size, sequence_length, 1)\n        x = x.squeeze(-1)  # (batch_size, sequence_length)\n        \n        # Cast the input to long (integer) type for the embedding layer\n        x = x.long()  # Convert to long (integer) tensor\n        \n        x = self.embedding(x)  # (batch_size, sequence_length, embedding_dim)\n        \n        # Pass through performer\n        x = self.performer(x)  # (batch_size, sequence_length, embedding_dim)\n        \n        # Take the output of the last token (or perform pooling) for classification\n        x = x.mean(dim=1)  # Mean pooling over the sequence\n        \n        # Pass through fully connected layer\n        x = self.fc(x)  # (batch_size, num_classes)\n        \n        return x\n\n# Set device to GPU (use GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = PerformerModel(vocab_size=256, embedding_dim=64).to(device)\n\n# Initialize loss function and optimizer\ncriterion = nn.CrossEntropyLoss()  # Binary classification (2 classes)\noptimizer = optim.Adam(model.parameters(), lr=1e-5)  # Try even lower learning rate\n\n# Set up mixed precision training\nscaler = torch.cuda.amp.GradScaler()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 5\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    \n    # Using tqdm for the progress bar in the training loop\n    with tqdm(train_dataloader1, unit=\"batch\", ncols=100, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n        for images, labels in pbar:\n            images, labels = images.to(device), labels.to(device)  # Move to device\n            \n            # Check for NaN values in images and labels\n            if torch.any(torch.isnan(images)) or torch.any(torch.isnan(labels)):\n                print(\"NaN detected in input data!\")\n                continue\n            \n            # Forward pass with mixed precision\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():  # Enable mixed precision\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            \n            # Check if loss is NaN\n            if torch.isnan(loss):\n                print(f\"NaN detected in loss at epoch {epoch+1}, batch {pbar.n}\")\n                continue\n            \n            # Backward pass and optimization with scaled gradients\n            scaler.scale(loss).backward()\n\n            # Gradient clipping (to prevent exploding gradients)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            # Update gradients with optimizer step\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            \n            # Update the progress bar with the current loss\n            pbar.set_postfix(loss=running_loss / (pbar.n + 1), refresh=True)\n    \n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dataloader)}\")\n\n# Test the model\nmodel.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_dataloader1:\n        images, labels = images.to(device), labels.to(device)  # Move to device\n        outputs = model(images)\n        predicted = torch.argmax(outputs, dim=1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 5\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    \n    # Using tqdm for the progress bar in the training loop\n    with tqdm(train_dataloader2, unit=\"batch\", ncols=100, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n        for images, labels in pbar:\n            images, labels = images.to(device), labels.to(device)  # Move to device\n            \n            # Check for NaN values in images and labels\n            if torch.any(torch.isnan(images)) or torch.any(torch.isnan(labels)):\n                print(\"NaN detected in input data!\")\n                continue\n            \n            # Forward pass with mixed precision\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():  # Enable mixed precision\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            \n            # Check if loss is NaN\n            if torch.isnan(loss):\n                print(f\"NaN detected in loss at epoch {epoch+1}, batch {pbar.n}\")\n                continue\n            \n            # Backward pass and optimization with scaled gradients\n            scaler.scale(loss).backward()\n\n            # Gradient clipping (to prevent exploding gradients)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            # Update gradients with optimizer step\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            \n            # Update the progress bar with the current loss\n            pbar.set_postfix(loss=running_loss / (pbar.n + 1), refresh=True)\n    \n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dataloader)}\")\n\n# Test the model\nmodel.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_dataloader2:\n        images, labels = images.to(device), labels.to(device)  # Move to device\n        outputs = model(images)\n        predicted = torch.argmax(outputs, dim=1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 5\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    \n    # Using tqdm for the progress bar in the training loop\n    with tqdm(train_dataloader3, unit=\"batch\", ncols=100, desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n        for images, labels in pbar:\n            images, labels = images.to(device), labels.to(device)  # Move to device\n            \n            # Check for NaN values in images and labels\n            if torch.any(torch.isnan(images)) or torch.any(torch.isnan(labels)):\n                print(\"NaN detected in input data!\")\n                continue\n            \n            # Forward pass with mixed precision\n            optimizer.zero_grad()\n            with torch.cuda.amp.autocast():  # Enable mixed precision\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n            \n            # Check if loss is NaN\n            if torch.isnan(loss):\n                print(f\"NaN detected in loss at epoch {epoch+1}, batch {pbar.n}\")\n                continue\n            \n            # Backward pass and optimization with scaled gradients\n            scaler.scale(loss).backward()\n\n            # Gradient clipping (to prevent exploding gradients)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            # Update gradients with optimizer step\n            scaler.step(optimizer)\n            scaler.update()\n            \n            running_loss += loss.item()\n            \n            # Update the progress bar with the current loss\n            pbar.set_postfix(loss=running_loss / (pbar.n + 1), refresh=True)\n    \n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_dataloader)}\")\n\n# Test the model\nmodel.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_dataloader3:\n        images, labels = images.to(device), labels.to(device)  # Move to device\n        outputs = model(images)\n        predicted = torch.argmax(outputs, dim=1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = correct / total\nprint(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}