{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10138003,"sourceType":"datasetVersion","datasetId":6256839},{"sourceId":10139303,"sourceType":"datasetVersion","datasetId":6257773},{"sourceId":10139667,"sourceType":"datasetVersion","datasetId":6258059}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport h5py\nimport timm\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nclass SequenceDataset(Dataset):\n    def __init__(self, h5_file):\n        self.h5_file = h5py.File(h5_file, \"r\")\n        self.images = self.h5_file[\"images\"]  # Shape: (N, 32, 32)\n        self.labels = self.h5_file[\"labels\"]  # Shape: (N,)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]  # Shape: (32, 32)\n        image = torch.tensor(image, dtype=torch.float32).unsqueeze(0)  # Shape: (1, 32, 32)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return image, label\n\nclass BEiT(nn.Module):\n    def __init__(self, img_size=32, in_channels=1, num_classes=2):\n        super(TinyBEiT, self).__init__()\n        self.model = timm.create_model(\n            'beit_base_patch16_224', \n            img_size=img_size,\n            patch_size=1,\n            in_chans=in_channels,\n            num_classes=num_classes,\n            embed_dim=64,\n            depth=4,\n            num_heads=2,\n            mlp_ratio=2.0,\n            drop_rate=0.1,\n            pretrained=False\n        )\n\n    def forward(self, x):\n        return self.model(x)\n    \n    @property\n    def config(self):\n        return {\n            'name': 'TinyBEiT-p1',\n            'embed_dim': self.model.embed_dim,\n            'depth': len(self.model.blocks),\n            'num_heads': self.model.blocks[0].attn.num_heads,\n            'num_params': sum(p.numel() for p in self.parameters())\n        }\n\n# Load dataset\nh5_file_path = \"/kaggle/input/easydata/merged_data (3).h5\"\ndataset = SequenceDataset(h5_file_path)\n\n# Split dataset\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create DataLoader with multiple workers\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n\n# Initialize the model\nmodel = TinyBEiT(img_size=32, in_channels=1, num_classes=2)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=1e-4)\n\n# Use mixed precision training for faster execution\nscaler = torch.cuda.amp.GradScaler()\n\n# Training loop\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass with mixed precision\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        # Backward pass and optimization with scaler\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n\n# Evaluation loop\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predictions = torch.max(outputs, dim=1)\n\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Test Accuracy: {correct / total:.4f}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T12:59:32.553667Z","iopub.execute_input":"2024-12-08T12:59:32.553908Z","iopub.status.idle":"2024-12-08T13:31:44.751676Z","shell.execute_reply.started":"2024-12-08T12:59:32.553882Z","shell.execute_reply":"2024-12-08T13:31:44.750594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"h5_file_path1 = \"/kaggle/input/mediumdata/merged_data (2).h5\"\ndataset1 = SequenceDataset(h5_file_path1)\n\n# Split dataset\ntrain_size1 = int(0.8 * len(dataset1))\ntest_size1 = len(dataset1) - train_size1\ntrain_dataset1, test_dataset1 = random_split(dataset1, [train_size1, test_size1])\n\n# Create DataLoader with multiple workers\ntrain_loader1 = DataLoader(train_dataset1, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader1 = DataLoader(test_dataset1, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n\n\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for images, labels in tqdm(train_loader1, desc=f\"Epoch {epoch+1}/{epochs}\"):\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass with mixed precision\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        # Backward pass and optimization with scaler\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n\n# Evaluation loop\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader1:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predictions = torch.max(outputs, dim=1)\n\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Test Accuracy: {correct / total:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:06:53.186406Z","iopub.execute_input":"2024-12-08T14:06:53.187249Z","iopub.status.idle":"2024-12-08T14:39:02.936133Z","shell.execute_reply.started":"2024-12-08T14:06:53.187212Z","shell.execute_reply":"2024-12-08T14:39:02.934932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"h5_file_path2 = \"/kaggle/input/harddata/merged_data (1).h5\"\ndataset2 = SequenceDataset(h5_file_path2)\n\n# Split dataset\ntrain_size2 = int(0.8 * len(dataset2))\ntest_size2 = len(dataset2) - train_size2\ntrain_dataset2, test_dataset2 = random_split(dataset2, [train_size2, test_size2])\n\n# Create DataLoader with multiple workers\ntrain_loader2 = DataLoader(train_dataset2, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\ntest_loader2 = DataLoader(test_dataset2, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n\n\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for images, labels in tqdm(train_loader2, desc=f\"Epoch {epoch+1}/{epochs}\"):\n        images, labels = images.to(device), labels.to(device)\n\n        # Forward pass with mixed precision\n        with torch.cuda.amp.autocast():\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        # Backward pass and optimization with scaler\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n\n# Evaluation loop\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_loader2:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predictions = torch.max(outputs, dim=1)\n\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\nprint(f\"Test Accuracy: {correct / total:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:46:46.844804Z","iopub.execute_input":"2024-12-08T14:46:46.845124Z","iopub.status.idle":"2024-12-08T15:18:56.824940Z","shell.execute_reply.started":"2024-12-08T14:46:46.845099Z","shell.execute_reply":"2024-12-08T15:18:56.823810Z"}},"outputs":[],"execution_count":null}]}