{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10143420,"sourceType":"datasetVersion","datasetId":6260904},{"sourceId":10143553,"sourceType":"datasetVersion","datasetId":6260998}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nimport h5py\nimport timm\nimport numpy as np\nfrom scipy import ndimage\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport torchmetrics\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\nfrom pytorch_lightning.loggers import TensorBoardLogger\n# Add these imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pytorch_lightning.callbacks import RichProgressBar, LearningRateMonitor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:59:34.703947Z","iopub.execute_input":"2024-12-09T02:59:34.705047Z","iopub.status.idle":"2024-12-09T02:59:34.710248Z","shell.execute_reply.started":"2024-12-09T02:59:34.705013Z","shell.execute_reply":"2024-12-09T02:59:34.709338Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"class PathEnhancer:\n    \"\"\"Enhances path features in images.\"\"\"\n    def __init__(self):\n        self.kernel = torch.ones(3, 3)\n        \n    def find_endpoints(self, image):\n        \"\"\"Locate the two brightest points (endpoints).\"\"\"\n        flat_image = image.view(-1)\n        values, indices = torch.topk(flat_image, 2)\n        return indices\n    \n    def get_path_direction(self, image):\n        \"\"\"Compute directional features for path pixels.\"\"\"\n        directions = torch.zeros((32, 32, 8))\n        padded = F.pad(image, (1, 1, 1, 1))\n        \n        for i in range(32):\n            for j in range(32):\n                if image[i, j] > 0.5:\n                    for idx, (di, dj) in enumerate([(-1,-1), (-1,0), (-1,1), (0,-1), \n                                                  (0,1), (1,-1), (1,0), (1,1)]):\n                        directions[i, j, idx] = padded[i+di+1, j+dj+1]\n        return directions\n    \n    def enhance_paths(self, image):\n        # Binarize\n        binary = (image > 127.5).float()\n        \n        # Find endpoints\n        endpoints = self.find_endpoints(binary)\n        \n        # Get path directions\n        directions = self.get_path_direction(binary)\n        \n        # Create distance transform from endpoints\n        endpoint_distances = torch.zeros((2, 32, 32))\n        coords = torch.stack(torch.meshgrid(torch.arange(32), torch.arange(32))).reshape(2, -1).t().float()  # Convert to float\n        for i, endpoint in enumerate(endpoints):\n            y, x = endpoint // 32, endpoint % 32\n            distances = torch.cdist(\n                torch.tensor([[float(y), float(x)]]),  # Convert to float\n                coords\n            ).reshape(32, 32)\n            endpoint_distances[i] = distances\n        \n        # Combine features\n        enhanced = torch.cat([\n            binary.unsqueeze(0),\n            directions.permute(2, 0, 1),\n            endpoint_distances,\n        ])\n        \n        return enhanced","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:59:34.711949Z","iopub.execute_input":"2024-12-09T02:59:34.712574Z","iopub.status.idle":"2024-12-09T02:59:34.729755Z","shell.execute_reply.started":"2024-12-09T02:59:34.712524Z","shell.execute_reply":"2024-12-09T02:59:34.729109Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"class PathfinderDataModule(pl.LightningDataModule):\n    def __init__(self, data_path, batch_size=64):\n        super().__init__()\n        self.data_path = data_path\n        self.batch_size = batch_size\n        self.enhancer = PathEnhancer()\n        \n    def setup(self, stage=None):\n        with h5py.File(self.data_path, 'r') as f:\n            total = len(f['images'])\n            indices = torch.randperm(total)\n            train_size = int(0.8 * total)\n            val_size = int(0.1 * total)\n            \n            self.train_indices = indices[:train_size]\n            self.val_indices = indices[train_size:train_size+val_size]\n            self.test_indices = indices[train_size+val_size:]\n            \n            print(f\"\\nDataset Statistics:\")\n            print(f\"Total samples: {total:,}\")\n            print(f\"Train: {len(self.train_indices):,}\")\n            print(f\"Val: {len(self.val_indices):,}\")\n            print(f\"Test: {len(self.test_indices):,}\")\n            \n            # Compute class weights for balanced training\n            labels = f['labels'][:]\n            pos_weight = (labels == 0).sum() / (labels == 1).sum()\n            self.pos_weight = torch.tensor([pos_weight])\n    \n    def _get_dataset(self, indices):\n        return EnhancedPathfinderDataset(self.data_path, indices, self.enhancer)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self._get_dataset(self.train_indices),\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=0\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self._get_dataset(self.val_indices),\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=0\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self._get_dataset(self.test_indices),\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=0\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:59:34.730931Z","iopub.execute_input":"2024-12-09T02:59:34.731253Z","iopub.status.idle":"2024-12-09T02:59:34.746632Z","shell.execute_reply.started":"2024-12-09T02:59:34.731216Z","shell.execute_reply":"2024-12-09T02:59:34.745951Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"class EnhancedPathfinderDataset(Dataset):\n    def __init__(self, h5_path, indices, enhancer):\n        self.h5_path = h5_path\n        self.indices = indices\n        self.enhancer = enhancer\n        \n        with h5py.File(h5_path, 'r') as f:\n            self.length = len(indices)\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, idx):\n        true_idx = self.indices[idx]\n        with h5py.File(self.h5_path, 'r') as f:\n            image = torch.from_numpy(f['images'][true_idx]).float()\n            label = torch.tensor(f['labels'][true_idx]).long()\n        \n        # Apply path enhancement\n        enhanced = self.enhancer.enhance_paths(image)\n        \n        return {\n            'image': enhanced,\n            'original': image,\n            'label': label\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:59:34.748243Z","iopub.execute_input":"2024-12-09T02:59:34.748801Z","iopub.status.idle":"2024-12-09T02:59:34.761366Z","shell.execute_reply.started":"2024-12-09T02:59:34.748775Z","shell.execute_reply":"2024-12-09T02:59:34.760584Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"class SequentialDeiT(pl.LightningModule):\n    def __init__(\n        self,\n        learning_rate=1e-4,\n        weight_decay=0.01,\n        warmup_steps=100\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Modified DeiT for sequential processing\n        self.model  = timm.create_model(\n            'deit_tiny_patch16_224',  # Change to tiny from small\n            pretrained=True,\n            num_classes=2,\n            img_size=32,\n            in_chans=11,\n            patch_size=8  # Increase from 4\n        )\n        \n        # Add path-specific attention\n        self.path_attention = nn.MultiheadAttention(\n            embed_dim=192,  # DeiT small hidden dim\n            num_heads=6,\n            dropout=0.1\n        )\n        \n        # Metrics\n        metrics = torchmetrics.MetricCollection({\n            'accuracy': torchmetrics.Accuracy(task='binary'),\n            'precision': torchmetrics.Precision(task='binary'),\n            'recall': torchmetrics.Recall(task='binary'),\n            'f1': torchmetrics.F1Score(task='binary')\n        })\n        \n        self.train_metrics = metrics.clone(prefix='train_')\n        self.val_metrics = metrics.clone(prefix='val_')\n        self.test_metrics = metrics.clone(prefix='test_')\n        \n    def forward(self, x):\n        # Process through DeiT\n        features = self.model.forward_features(x)\n        \n        # Add path-specific attention\n        attn_out, _ = self.path_attention(\n            features, features, features,\n            need_weights=False\n        )\n        \n        # Combine with original features\n        features = features + attn_out\n        \n        # Classification head\n        return self.model.head(features.mean(1))\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch['image'], batch['label']\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(1) == y).float().mean()\n        \n        # Modify metrics calculation\n        self.train_metrics(logits.argmax(dim=1), y)  # Use predicted class instead of probabilities\n        self.log('train_loss', loss, prog_bar=True)\n        self.log('train_acc', acc, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch['image'], batch['label']\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(1) == y).float().mean()\n        \n        self.val_metrics(logits.argmax(dim=1), y)\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch['image'], batch['label']\n        logits = self(x)\n        acc = (logits.argmax(1) == y).float().mean()\n        \n        self.test_metrics(logits.argmax(dim=1), y)\n        self.log('test_acc', acc, prog_bar=True)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.learning_rate,\n            weight_decay=self.hparams.weight_decay\n        )\n        \n        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n            optimizer,\n            max_lr=self.hparams.learning_rate,\n            total_steps=self.trainer.estimated_stepping_batches,\n            pct_start=0.1\n        )\n        \n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:59:34.839094Z","iopub.execute_input":"2024-12-09T02:59:34.839372Z","iopub.status.idle":"2024-12-09T02:59:34.851340Z","shell.execute_reply.started":"2024-12-09T02:59:34.839348Z","shell.execute_reply":"2024-12-09T02:59:34.850599Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"def train_progressive_deit():\n   print(f\"GPU available: {torch.cuda.is_available()}\")\n   if torch.cuda.is_available():\n       print(f\"Device: {torch.cuda.get_device_name()}\")\n\n   # Training history storage\n   history = {\n       'medium_train_loss': [], 'medium_val_loss': [],\n       'medium_train_acc': [], 'medium_val_acc': [],\n       'hard_train_loss': [], 'hard_val_loss': [],\n       'hard_train_acc': [], 'hard_val_acc': []\n   }\n\n   # Medium dataset training\n   data_path_medium = '/kaggle/input/deit-data/merged_data_medium.h5'\n   datamodule_medium = PathfinderDataModule(data_path_medium, batch_size=64)\n   model = SequentialDeiT()\n   \n   trainer_medium = pl.Trainer(\n       max_epochs=5,\n       accelerator='gpu',\n       devices=1,\n       precision=16,\n       limit_train_batches=0.5,\n       limit_val_batches=0.3,\n       callbacks=[\n           ModelCheckpoint(\n               dirpath='checkpoints/medium',\n               filename='{epoch}-{val_accuracy:.2f}',\n               monitor='val_accuracy',\n               mode='max'\n           ),\n           EarlyStopping(monitor='val_loss', patience=3),\n           RichProgressBar(),\n           LearningRateMonitor(logging_interval='step')\n       ],\n       logger=TensorBoardLogger(\"logs\", name=\"deit_medium\")\n   )\n   \n   print(\"\\n\" + \"=\"*50)\n   print(\"Training on Medium Dataset...\")\n   print(\"=\"*50)\n   trainer_medium.fit(model, datamodule_medium)\n   \n   # Store medium dataset metrics\n   history['medium_train_loss'] = trainer_medium.callback_metrics['train_loss'].item()\n   history['medium_val_loss'] = trainer_medium.callback_metrics['val_loss'].item()\n   history['medium_train_acc'] = trainer_medium.callback_metrics['train_acc'].item()\n   history['medium_val_acc'] = trainer_medium.callback_metrics['val_acc'].item()\n   \n   # Hard dataset training\n   data_path_hard = '/kaggle/input/deit-data/merged_data_hard.h5'\n   datamodule_hard = PathfinderDataModule(data_path_hard, batch_size=64)\n   \n   trainer_hard = pl.Trainer(\n       max_epochs=5,\n       accelerator='gpu',\n       devices=1,\n       precision=16,\n       limit_train_batches=0.5,\n       limit_val_batches=0.3,\n       callbacks=[\n           ModelCheckpoint(\n               dirpath='checkpoints/hard',\n               filename='{epoch}-{val_accuracy:.2f}',\n               monitor='val_accuracy',\n               mode='max'\n           ),\n           EarlyStopping(monitor='val_loss', patience=3),\n           RichProgressBar(),\n           LearningRateMonitor(logging_interval='step')\n       ],\n       logger=TensorBoardLogger(\"logs\", name=\"deit_hard\")\n   )\n   \n   print(\"\\n\" + \"=\"*50)\n   print(\"Training on Hard Dataset...\")\n   print(\"=\"*50)\n   trainer_hard.fit(model, datamodule_hard)\n   \n   # Store hard dataset metrics\n   history['hard_train_loss'] = trainer_hard.callback_metrics['train_loss'].item()\n   history['hard_val_loss'] = trainer_hard.callback_metrics['val_loss'].item()\n   history['hard_train_acc'] = trainer_hard.callback_metrics['train_acc'].item()\n   history['hard_val_acc'] = trainer_hard.callback_metrics['val_acc'].item()\n   \n   # Plot training history\n   plot_training_history(history)\n   \n   return model, history\n\ndef plot_training_history(history):\n   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n   \n   # Loss plot\n   ax1.plot(history['medium_train_loss'], label='Medium Train')\n   ax1.plot(history['medium_val_loss'], label='Medium Val')\n   ax1.plot(history['hard_train_loss'], label='Hard Train')\n   ax1.plot(history['hard_val_loss'], label='Hard Val')\n   ax1.set_title('Loss History')\n   ax1.set_xlabel('Epoch')\n   ax1.set_ylabel('Loss')\n   ax1.legend()\n   ax1.grid(True)\n   \n   # Accuracy plot\n   ax2.plot(history['medium_train_acc'], label='Medium Train')\n   ax2.plot(history['medium_val_acc'], label='Medium Val')\n   ax2.plot(history['hard_train_acc'], label='Hard Train')\n   ax2.plot(history['hard_val_acc'], label='Hard Val')\n   ax2.set_title('Accuracy History')\n   ax2.set_xlabel('Epoch')\n   ax2.set_ylabel('Accuracy')\n   ax2.legend()\n   ax2.grid(True)\n   \n   plt.tight_layout()\n   plt.savefig('training_history.png')\n   plt.show()\n\n# Run training with TensorBoard\nmodel, history = train_progressive_deit()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T02:59:34.853225Z","iopub.execute_input":"2024-12-09T02:59:34.853997Z","iopub.status.idle":"2024-12-09T03:05:11.754739Z","shell.execute_reply.started":"2024-12-09T02:59:34.853956Z","shell.execute_reply":"2024-12-09T03:05:11.753173Z"}},"outputs":[{"name":"stdout","text":"GPU available: True\nDevice: Tesla P100-PCIE-16GB\n\n==================================================\nTraining on Medium Dataset...\n==================================================\n\nDataset Statistics:\nTotal samples: 200,000\nTrain: 160,000\nVal: 20,000\nTest: 20,000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType              \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\n┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model          │ VisionTransformer  │  5.5 M │ train │\n│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ path_attention │ MultiheadAttention │  148 K │ train │\n│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ train_metrics  │ MetricCollection   │      0 │ train │\n│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ val_metrics    │ MetricCollection   │      0 │ train │\n│\u001b[2m \u001b[0m\u001b[2m4\u001b[0m\u001b[2m \u001b[0m│ test_metrics   │ MetricCollection   │      0 │ train │\n└───┴────────────────┴────────────────────┴────────┴───────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type               </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃\n┡━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model          │ VisionTransformer  │  5.5 M │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ path_attention │ MultiheadAttention │  148 K │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ train_metrics  │ MetricCollection   │      0 │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ val_metrics    │ MetricCollection   │      0 │ train │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>│ test_metrics   │ MetricCollection   │      0 │ train │\n└───┴────────────────┴────────────────────┴────────┴───────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mTrainable params\u001b[0m: 5.6 M                                                                                            \n\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n\u001b[1mTotal params\u001b[0m: 5.6 M                                                                                                \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 22                                                                         \n\u001b[1mModules in train mode\u001b[0m: 281                                                                                         \n\u001b[1mModules in eval mode\u001b[0m: 0                                                                                            \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 5.6 M                                                                                            \n<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total params</span>: 5.6 M                                                                                                \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 22                                                                         \n<span style=\"font-weight: bold\">Modules in train mode</span>: 281                                                                                         \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 0                                                                                            \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec7cfe166434049b310d4def3471ee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:212\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    211\u001b[0m dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m batch, _, __ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:78\u001b[0m, in \u001b[0;36m_MaxSizeCycle.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     out[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","Cell \u001b[0;32mIn[100], line 20\u001b[0m, in \u001b[0;36mEnhancedPathfinderDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Apply path enhancement\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m enhanced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menhancer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menhance_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: enhanced,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m: image,\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label\n\u001b[1;32m     26\u001b[0m }\n","Cell \u001b[0;32mIn[98], line 33\u001b[0m, in \u001b[0;36mPathEnhancer.enhance_paths\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Get path directions\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m directions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path_direction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Create distance transform from endpoints\u001b[39;00m\n","Cell \u001b[0;32mIn[98], line 19\u001b[0m, in \u001b[0;36mPathEnhancer.get_path_direction\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image[i, j] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, (di, dj) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \n\u001b[1;32m     21\u001b[0m                                       (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)]):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[102], line 122\u001b[0m\n\u001b[1;32m    119\u001b[0m    plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Run training with TensorBoard\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_progressive_deit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[102], line 43\u001b[0m, in \u001b[0;36mtrain_progressive_deit\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on Medium Dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtrainer_medium\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule_medium\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Store medium dataset metrics\u001b[39;00m\n\u001b[1;32m     46\u001b[0m history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedium_train_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m trainer_medium\u001b[38;5;241m.\u001b[39mcallback_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n","\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"],"ename":"NameError","evalue":"name 'exit' is not defined","output_type":"error"}],"execution_count":102},{"cell_type":"code","source":"# View in TensorBoard (separate cell)\n%load_ext tensorboard\n%tensorboard --logdir logs/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:05:11.755586Z","iopub.status.idle":"2024-12-09T03:05:11.756044Z","shell.execute_reply.started":"2024-12-09T03:05:11.755795Z","shell.execute_reply":"2024-12-09T03:05:11.755818Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir logs/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T03:05:11.757351Z","iopub.status.idle":"2024-12-09T03:05:11.757636Z","shell.execute_reply.started":"2024-12-09T03:05:11.757501Z","shell.execute_reply":"2024-12-09T03:05:11.757515Z"}},"outputs":[],"execution_count":null}]}